# LLM training


## Introduction

Generative AI (GAI) offers unprecedented opportunities for research and innovation, but its commercialization has raised concerns about transparency, reproducibility, and safety. Many open GAI models lack the necessary components for full understanding and reproducibility, and some use restrictive licenses whilst claiming to be “open-source”. To address these concerns, we follow the [Model Openness Framework (MOF)](https://arxiv.org/pdf/2403.13784), a ranked classification system that rates machine learning models based on their completeness and openness, following principles of open science, open source, open data, and open access. 

By promoting transparency and reproducibility, the MOF combats “openwashing” practices and establishes completeness and openness as primary criteria alongside the core tenets of responsible AI. Wide adoption of the MOF will foster a more open AI ecosystem, benefiting research, innovation, and adoption of state-of-the-art models. 

We follow MOF to release the datasets during training, the training scripts, and the trained models. 



## Environment


## Model
You can download our efficient stable diffusion model from this [link](https://huggingface.co/piuzha/llm_ckpts). It is located on Huggingface with 'piuzha/llm_ckpts'.

## Usage


## Timeline


Timeline for OminiX for Stable Diffusion development and open-source

| Time          	| Task                                                                           	| Open Source version                                                   	|
|---------------	|--------------------------------------------------------------------------------	|-----------------------------------------------------------------------	|
| 05/21         	|                                                                                	| The first open source: Current version of the LLM training checkpoint 	|
| 05/21 – 07/15 	| Distributed training of the LLM                                                	| 07/16 Training checkpoint, data, and evaluation results               	|
| 06/15 – 07/15 	| Schedule with Linux Foundation about PR                                        	| Schedule with Linux Foundation                                        	|
| 07/16         	| Reinformcement learning based finetuning, distillation for smaller model, etc. 	| TBM                                                                   	|

